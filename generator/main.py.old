import os
import glob
import json
import shutil
import re
import yaml
import numpy as np
from sklearn.decomposition import PCA
from PIL import Image
import torch
from transformers import CLIPProcessor, CLIPModel
import markdown
import argparse

# Function to clear output directory
def clear_output_dir(output_dir):
    if os.path.exists(output_dir):
        shutil.rmtree(output_dir)
    os.makedirs(output_dir)
    os.makedirs(os.path.join(output_dir, 'content'))
    os.makedirs(os.path.join(output_dir, 'nodes'))
    print(f"Cleared and created output directory structure at {output_dir}")

# Function to load and embed markdown files
def load_markdown_files(path):
    files = glob.glob(os.path.join(path, "*.md"))
    contents = [open(file, 'r').read() for file in files]
    print(f"Loaded {len(files)} markdown files from {path}")
    return files, contents

# Function to load images
def load_images(path):
    files = glob.glob(os.path.join(path, "*.png")) + glob.glob(os.path.join(path, "*.jpg"))
    images = [Image.open(file) for file in files]
    print(f"Loaded {len(files)} images from {path}")
    return files, images

# Function to extract metadata from markdown content
def extract_metadata(content):
    # Extract YAML metadata block
    yaml_block = re.search(r'---(.*?)---', content, re.DOTALL)
    yaml_data = {}
    if yaml_block:
        try:
            yaml_content = yaml_block.group(1)
            yaml_data = yaml.safe_load(yaml_content)
            content = content.replace(yaml_block.group(0), '')
        except Exception as e:
            yaml_data = {}
            print('failed to extract yaml_data:', str(e))

    if yaml_data is None:
        yaml_data = {}

    # Extract links and image links
    links = re.findall(r'!\[\[(.*?)\|?(.*?)\]\]|\[\[(.*?)\|?(.*?)\]\]', content)
    links = [link for group in links for link in group if link]

    # Extract tags
    tags = re.findall(r'#([a-zA-Z0-9_/-]+)', content)

    print(yaml_data)

    # Combine YAML tags with inline tags
    if 'tags' in yaml_data:
        yaml_data['tags'] = list(set(yaml_data['tags'] + tags))
    else:
        yaml_data['tags'] = tags

    metadata = {
        'links': links,
        'tags': yaml_data.get('tags', [])
    }
    yaml_data.pop('tags', None)
    metadata.update(yaml_data)
    
    return content, metadata

# Embedding function using CLIP
def embed_with_clip(texts, images):
    device = "mps" if torch.backends.mps.is_available() else "cpu"
    model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32").to(device)
    processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
    print("Model and processor loaded to device:", device)

    # Embed texts
    text_inputs = processor(text=texts, return_tensors="pt", padding=True, truncation=True).to(device)
    text_embeddings = model.get_text_features(**text_inputs)
    print(f"Text embeddings computed for {len(texts)} texts")

    # Embed images
    image_inputs = processor(images=images, return_tensors="pt").to(device)
    image_embeddings = model.get_image_features(**image_inputs)
    print(f"Image embeddings computed for {len(images)} images")

    return text_embeddings.cpu().detach().numpy(), image_embeddings.cpu().detach().numpy()

# Dimensionality reduction to 3D
def reduce_to_3d(embeddings):
    pca = PCA(n_components=3)
    reduced_embeddings = pca.fit_transform(embeddings)
    print(f"Reduced embeddings to 3D space using PCA")
    return reduced_embeddings

# Function to generate HTML content
def generate_html(content, content_type):
    if content_type == 'markdown':
        html_content = markdown.markdown(content)
    else:
        html_content = f'<img src="content/{os.path.basename(content)}" alt="Image">'
    return html_content

# Function to copy images to content directory
def copy_images(files, output_dir):
    output_paths = []
    for file in files:
        filename = os.path.basename(file)
        dest = os.path.join(output_dir, 'content', filename)
        shutil.copy(file, dest)
        output_paths.append(dest)
    print(f"Copied {len(files)} images to {output_dir}/content")
    return output_paths

# Function to write HTML files
def write_html_files(files, contents, content_type, output_dir):
    paths = []
    for file, content in zip(files, contents):
        filename = os.path.splitext(os.path.basename(file))[0] + '.html'
        html_content = generate_html(content, content_type)
        path = os.path.join(output_dir, 'nodes', filename)
        with open(path, 'w') as f:
            f.write(html_content)
        paths.append(path)
    print(f"Generated HTML files for {len(files)} {content_type} contents")
    return paths

# Main function to create embeddings, HTML files, and JSON mapping
def process_content(input_path, output_dir):
    clear_output_dir(output_dir)

    md_files, md_contents = load_markdown_files(input_path)
    img_files, images = load_images(input_path)
    
    all_texts = []
    all_metadata = []
    for content in md_contents:
        cleaned_content, metadata = extract_metadata(content)
        all_texts.append(cleaned_content)
        all_metadata.append(metadata)

    all_texts += [""] * len(images)  # Empty texts for images
    
    text_embeddings, image_embeddings = embed_with_clip(all_texts, images)
    all_embeddings = np.vstack((text_embeddings[:len(md_files)], image_embeddings))
    
    reduced_embeddings = reduce_to_3d(all_embeddings)
    
    md_paths = write_html_files(md_files, all_texts, 'markdown', output_dir)
    img_paths = copy_images(img_files, output_dir)
    img_html_paths = write_html_files(img_files, img_paths, 'image', output_dir)
    
    data = {}
    for i, (file, metadata) in enumerate(zip(md_files, all_metadata)):
        filename = os.path.basename(file)
        relative_path = os.path.relpath(md_paths[i], output_dir)
        data[filename] = {
            'embedding': reduced_embeddings[i].tolist(),
            'filetype': 'markdown',
            'metadata': metadata,
            'path': '/'+relative_path
        }
    
    for i, file in enumerate(img_files):
        filename = os.path.basename(file)
        relative_path = os.path.relpath(img_html_paths[i], output_dir)
        data[filename] = {
            'embedding': reduced_embeddings[len(md_files) + i].tolist(),
            'filetype': 'image',
            'metadata': {},
            'path': '/'+relative_path
        }
    
    with open(os.path.join(output_dir, 'nodes.json'), 'w') as json_file:
        json.dump(data, json_file, indent=4)
    print(f"Saved JSON data to {os.path.join(output_dir, 'nodes.json')}")

# Run the main function with arguments
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Process markdown and image content.")
    parser.add_argument("input_path", type=str, help="Path to the input content directory.")
    parser.add_argument("output_dir", type=str, help="Path to the output directory.")
    args = parser.parse_args()
    process_content(args.input_path, args.output_dir)
